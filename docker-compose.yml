services:
  hadoop-namenode:
    container_name: hadoop-namenode
    image: apache/hadoop:3.4.1
    ports:
    - "9870:9870"
    volumes:
    - ./hadoop-config/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
    - ./hadoop-config/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml 
    command: ["bash", "-c", "hdfs namenode -format && hdfs namenode"]

  hadoop-datanode-1:
    container_name: hadoop-datanode-1
    image: apache/hadoop:3.4.1
    ports:
    - "9864:9864"
    volumes:
    - ./hadoop-config/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
    - ./hadoop-config/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml 
    command: ["bash", "-c", "hdfs datanode"]

  hadoop-datanode-2:
    container_name: hadoop-datanode-2
    image: apache/hadoop:3.4.1
    ports:
    - "9865:9864"
    volumes:
    - ./hadoop-config/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
    - ./hadoop-config/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml 
    command: ["bash", "-c", "hdfs datanode"]
  
  postgres-metastore:
    image: postgres:14
    container_name: postgres-metastore
    environment:
      - POSTGRES_DB=metastore_db
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=password
    ports:
      - "5332:5332"

  postgres-hue:
    image: postgres:14
    container_name: postgres-hue
    environment:
      POSTGRES_DB: hue
      POSTGRES_USER: hue
      POSTGRES_PASSWORD: hue
    ports:
      - "5543:5432"
  
  metastore:
    image: apache/hive:4.0.1
    container_name: metastore
    entrypoint: ["sh", "-c", "ln -sf /opt/hadoop/share/hadoop/tools/lib/hadoop-aws-3.3.6.jar /opt/hadoop/share/hadoop/common/lib/ && ln -sf /opt/hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.12.367.jar /opt/hadoop/share/hadoop/common/lib/ && sleep 3 && /entrypoint.sh"] #esperar a que el postgre estÃ© listo
    environment:
      - SERVICE_NAME=metastore
      - DB_DRIVER=postgres
      - SERVICE_OPTS=-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres-metastore:5432/metastore_db -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=password
    ports:
      - "9083:9083"
    volumes:
      - ./hadoop-config/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./hadoop-config/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - ./hive-config/hive-site.xml:/opt/hive/conf/hive-site.xml
      - ./postgres-driver/postgresql-42.7.6.jar:/opt/hive/lib/postgres.jar
    depends_on:
      - postgres-metastore

  hive:
    image: apache/hive:4.0.1
    container_name: hive-server
    entrypoint: ["sh", "-c", "ln -sf /opt/hadoop/share/hadoop/tools/lib/hadoop-aws-3.3.6.jar /opt/hadoop/share/hadoop/common/lib/ && ln -sf /opt/hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.12.367.jar /opt/hadoop/share/hadoop/common/lib/ && sleep 4 && /entrypoint.sh"] #esperar a que el postgre estÃ© listo
    environment:
      - SERVICE_NAME=hiveserver2
      - IS_RESUME=true #adicional por el metastore
      - SERVICE_OPTS=-Dhive.metastore.uris=thrift://metastore:9083 -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres-metastore:5432/metastore_db -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=password #adicional por el metastore
      - SKIP_SCHEMA_INIT=false
    depends_on:
      - hadoop-namenode
      - metastore #adicional por el metastore
    ports:
      - "10000:10000"  # JDBC
      - "10002:10002"  # Web UI / WebHCat
    volumes:
      - ./hadoop-config/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./hadoop-config/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - ./hive-config/hive-site.xml:/opt/hive/conf/hive-site.xml
      - ./postgres-driver/postgresql-42.7.6.jar:/opt/hive/lib/postgres.jar #ADICIONAL POR EL METASTORE
    
  hue:
    image: gethue/hue:latest
    container_name: hue 
    depends_on:
      - postgres-hue
    ports:
      - "8888:8888"
    volumes:
      - ./hue-config/hue.ini:/usr/share/hue/desktop/conf/hue.ini

  spark-master:
    image: apache/spark:latest 
    container_name: spark-master 
    command: bash -c "/opt/spark/bin/spark-class org.apache.spark.deploy.master.Master"
    ports:
      - "7077:7077"   # Puerto para workers
      - "8081:8080"   # Interfaz web del master
    volumes:
      - ./hadoop-config/core-site.xml:/opt/spark/conf/core-site.xml
    depends_on:
      - hadoop-namenode

  spark-worker-1:
    image: apache/spark:latest
    container_name: spark-worker-1
    command: bash -c "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077"
    ports:
      - "8082:8081"   # Interfaz web del worker
    volumes:
      - ./hadoop-config/core-site.xml:/opt/spark/conf/core-site.xml
    depends_on:
      - spark-master
    
  spark-worker-2:
    image: apache/spark:latest
    container_name: spark-worker-2
    command: bash -c "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077"
    ports:
      - "8083:8081"   # Interfaz web del worker
    volumes:
      - ./hadoop-config/core-site.xml:/opt/spark/conf/core-site.xml
    depends_on:
      - spark-master

  python-pyspark4-jupyter:
    image: python-pyspark4-jupyter
    container_name: python-pyspark4-jupyter
    ports:
      - "8084:8888"

  superset:
    image: apache/superset:latest
    container_name: superset
    user: root
    volumes:
      - ./superset_config.py:/app/pythonpath/superset_config.py
    ports:
      - "8088:8088"
    environment:
      - SUPERSET_LOAD_EXAMPLES=no
    depends_on:
      - hive
    command: >
      /bin/bash -c "
        apt-get update &&
        apt-get install -y build-essential gcc g++ python3-dev libsasl2-dev &&
        pip install pyhive thrift sasl thrift-sasl &&
        superset db upgrade &&
        superset fab create-admin --username admin --firstname admin --lastname admin --email admin@superset.com --password admin &&
        superset init &&
        superset run -h 0.0.0.0 -p 8088 --with-threads --reload --debugger
      "

    